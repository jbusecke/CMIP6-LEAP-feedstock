{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbdf091d-9e77-4b5b-9ac1-867246787baa",
   "metadata": {},
   "source": [
    "# Prototype retraction logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c840cfc0-fb54-4cf4-b78d-7b022b2b6c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m2/vwrjf3zx1bvdbhfkw7bxpz7r0000gn/T/ipykernel_18105/1536322670.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# from https://github.com/pangeo-data/pangeo-cmip6-cloud/blob/master/retractions.py\n",
    "from tqdm.autonotebook import tqdm\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def query_retraction(url, params, batchsize):\n",
    "    print(f\"Downloading Retraction Data from {url}...\")\n",
    "    resp = requests.get(url=url, params=params)\n",
    "    header = resp.json()  # Check the JSON Response Content documentation below\n",
    "    n_items = header[\"response\"][\"numFound\"]\n",
    "    print(f\"Found {n_items} items.\")\n",
    "\n",
    "    batches = range(0, n_items+1, batchsize)  # if I offset these, can\n",
    "    params[\"limit\"] = batchsize\n",
    "\n",
    "    batch_jsons = []\n",
    "\n",
    "    \n",
    "    for batch in tqdm(batches):\n",
    "        params[\"offset\"] = batch\n",
    "        resp = requests.get(url=url, params=params)\n",
    "        if resp.status_code != 200:\n",
    "            print(batch)\n",
    "            print(resp.status_code)\n",
    "        try:\n",
    "            data = resp.json()\n",
    "            batch_jsons.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"FAILED with {e}. {resp=}\")\n",
    "\n",
    "    # Convert to list of instance ids\n",
    "    print(\"Extracting instance_ids...\")\n",
    "    all_retracted_instance_ids = []\n",
    "    for data in batch_jsons:\n",
    "        extracted = [i[\"instance_id\"] for i in data[\"response\"][\"docs\"]]\n",
    "        all_retracted_instance_ids.extend(extracted)\n",
    "\n",
    "    # # Fail out here if the total number of items is not what was promised in the header\n",
    "    # # I had a few instances today, where that was the case, I think a simple retry is\n",
    "    # # a good enough solution for now.\n",
    "    # n_retracted = len(all_retracted_instance_ids)\n",
    "    # if n_retracted == n_items:\n",
    "    #     print('Successfully downloaded all retraction info')\n",
    "    # else:\n",
    "    #     raise RuntimeError(f'Downloaded retraction info is incomplete. Found {n_retracted} items, expected {n_items}')\n",
    "\n",
    "    # # There is the possibility that we are getting duplicate instance_ids here because we query replicas\n",
    "    # # Make sure dubplicates are not carried forward\n",
    "    # retracted_instance_ids = set(all_retracted_instance_ids)\n",
    "    # print(f\"{len(all_retracted_instance_ids)-len(retracted_instance_ids)} replicas found\")\n",
    "    # return retracted_instance_ids\n",
    "\n",
    "# def query_retraction_retry(url, params, batchsize = 10000):\n",
    "#     \"\"\"Retrys query if it fails\"\"\"\n",
    "#     status = 0\n",
    "#     while status == 0:\n",
    "#         try:\n",
    "#             query_result = query_retraction(url, params, batchsize)\n",
    "#             status = 1\n",
    "#         except RuntimeError as e:\n",
    "#             print(f\"{e}.\\nRetrying\")\n",
    "    \n",
    "#     return query_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf71160-40ab-44af-8172-fae87255060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "import pandas as pd\n",
    "import os\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b713c-6632-4dc8-8132-d4289fc86436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Retraction Data from https://esgf-data.dkrz.de/esg-search/search...\n",
      "Found 697522 items.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a768cdf39fb54e189e293ceed82d4543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gcs = gcsfs.GCSFileSystem()\n",
    "catalog_url = \"https://cmip6.storage.googleapis.com/pangeo-cmip6.csv\"\n",
    "node_urls = [\n",
    "# \"https://esgf-node.llnl.gov/esg-search/search\",\n",
    "\"https://esgf-data.dkrz.de/esg-search/search\",\n",
    "# \"https://esgf-index1.ceda.ac.uk/esg-search/search\",\n",
    "# \"https://esgf-node.ipsl.upmc.fr/esg-search/search\",\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"type\": \"Dataset\",\n",
    "    \"mip_era\": \"CMIP6\",\n",
    "    \"replica\": \"none\",\n",
    "    \"distrib\": \"true\",\n",
    "    \"retracted\": \"true\",\n",
    "    \"format\": \"application/solr+json\",\n",
    "    \"fields\": \"instance_id\",\n",
    "}\n",
    "# query every one of the nodes\n",
    "retracted_ids = {}\n",
    "for url in node_urls:\n",
    "    retracted_ids[url.split('.')[1]] = query_retraction(url, params, batchsize=1000)\n",
    "\n",
    "# convert to pandas dataframes\n",
    "retracted_ids_df = {k:pd.Series(list(v)).to_frame(name=\"instance_id\") for k,v in retracted_ids.items()}\n",
    "\n",
    "# iteratively merge dataframes with 'outer' to get all possible retractions\n",
    "# from https://stackoverflow.com/a/44338256\n",
    "retracted_df = reduce(\n",
    "    lambda  left,right: pd.merge(\n",
    "        left,\n",
    "        right,\n",
    "        on=['instance_id'],\n",
    "        how='outer'\n",
    "    ), \n",
    "    retracted_ids_df.values()\n",
    ")\n",
    "\n",
    "## document missing instances for each node\n",
    "print('Documenting missing instance_ids per node')\n",
    "def unique_instances(df, df_full):\n",
    "    \"\"\"Return all the items of `df_full` not found in `df`\"\"\"\n",
    "    df_merged = pd.merge(df, df_full, on=['instance_id'],how='outer', indicator=True)\n",
    "    df_merged = df_merged[df_merged['_merge']=='right_only']\n",
    "    df_merged = df_merged.drop(columns=['_merge'])\n",
    "    return df_merged\n",
    "\n",
    "missing_ids = {k: unique_instances(v, retracted_df) for k,v in retracted_ids_df.items()}\n",
    "\n",
    "for k,v in missing_ids.items():\n",
    "    print(f\"Found {len(v)} missing instances from the {k} node.\")\n",
    "    filename = f\"missing_instance_ids_{k}.csv\"\n",
    "    v['instance_id'].to_csv(filename, index=False)\n",
    "    print(f\"Missing instance_ids written to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417f133-2ac3-48c1-a51d-105897edc0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retracted_ids_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e21f97b-83a0-4ae0-b6f3-2e9aa4d6307d",
   "metadata": {},
   "source": [
    "## I need to async this, but too tired to debug chatgpt now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a543ee-1977-460e-b0f7-ed4967492e47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresp\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resp' is not defined"
     ]
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9260972-cffe-4055-8448-8a74d553290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Retraction Data from https://esgf-node.llnl.gov/esg-search/search...\n",
      "Found 696747 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                                                   | 187/6968 [00:50<07:02, 16.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696500\n",
      "500\n",
      "<!doctype html><html lang=\"en\"><head><title>HTTP Status 500 – Internal Server Error</title><style type=\"text/css\">body {font-family:Tahoma,Arial,sans-serif;} h1, h2, h3, b {color:white;background-color:#525D76;} h1 {font-size:22px;} h2 {font-size:16px;} h3 {font-size:14px;} p {font-size:12px;} a {color:black;} .line {height:1px;background-color:#525D76;border:none;}</style></head><body><h1>HTTP Status 500 – Internal Server Error</h1><hr class=\"line\" /><p><b>Type</b> Exception Report</p><p><b>Message</b> Read timed out</p><p><b>Description</b> The server encountered an unexpected condition that prevented it from fulfilling the request.</p><p><b>Exception</b></p><pre>java.net.SocketTimeoutException: Read timed out\n",
      "\tjava.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tjava.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tjava.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tjava.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tjava.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n",
      "\tjava.io.BufferedInputStream.read1(BufferedInputStream.java:286)\n",
      "\tjava.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tsun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:743)\n",
      "\tsun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678)\n",
      "\tsun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1595)\n",
      "\tsun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1500)\n",
      "\tesg.search.utils.HttpClient.getResponse(HttpClient.java:127)\n",
      "\tesg.search.utils.HttpClient.doPost(HttpClient.java:98)\n",
      "\tesg.search.query.impl.solr.SearchServiceImpl._query(SearchServiceImpl.java:234)\n",
      "\tesg.search.query.impl.solr.SearchServiceImpl.query(SearchServiceImpl.java:161)\n",
      "\tesg.search.query.ws.rest.BaseController.process(BaseController.java:223)\n",
      "\tesg.search.query.ws.rest.SearchController.search(SearchController.java:51)\n",
      "\tsun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)\n",
      "\tsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tjava.lang.reflect.Method.invoke(Method.java:498)\n",
      "\torg.springframework.web.bind.annotation.support.HandlerMethodInvoker.invokeHandlerMethod(HandlerMethodInvoker.java:181)\n",
      "\torg.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter.invokeHandlerMethod(AnnotationMethodHandlerAdapter.java:440)\n",
      "\torg.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter.handle(AnnotationMethodHandlerAdapter.java:428)\n",
      "\torg.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967)\n",
      "\torg.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901)\n",
      "\torg.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970)\n",
      "\torg.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861)\n",
      "\tjavax.servlet.http.HttpServlet.service(HttpServlet.java:655)\n",
      "\torg.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846)\n",
      "\tjavax.servlet.http.HttpServlet.service(HttpServlet.java:764)\n",
      "\torg.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)\n",
      "\torg.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)\n",
      "\torg.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)\n",
      "\torg.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)\n",
      "\torg.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n",
      "\torg.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)\n",
      "\torg.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n",
      "\torg.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)\n",
      "\torg.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n",
      "\torg.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)\n",
      "\torg.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n",
      "\torg.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)\n",
      "\torg.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n",
      "\torg.springframework.security.web.authentication.preauth.AbstractPreAuthenticatedProcessingFilter.doFilter(AbstractPreAuthenticatedProcessingFilter.java:121)\n",
      "\torg.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n",
      "\torg.springframework.security.web.csrf.CsrfFilter.doFilterInternal(CsrfFilter.java:100)\n",
      "\torg.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n",
      "\torg.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n",
      "\torg.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:66)\n",
      "\torg.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n",
      "\torg.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n",
      "\torg.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)\n",
      "\torg.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)\n",
      "\torg.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n",
      "\torg.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)\n",
      "\torg.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)\n",
      "\torg.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)\n",
      "\torg.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)\n",
      "\torg.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)\n",
      "\torg.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)\n",
      "</pre><p><b>Note</b> The full stack trace of the root cause is available in the server logs.</p><hr class=\"line\" /><h3>Apache Tomcat/8.5.81</h3></body></html>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"JSONDecodeError\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m, in \u001b[0;36mfetch_data\u001b[0;34m(session, url, params)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(raw_response)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniforge3/envs/cmip6-leap-feedstock-maintenance/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/cmip6-leap-feedstock-maintenance/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/miniforge3/envs/cmip6-leap-feedstock-maintenance/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 68\u001b[0m\n\u001b[1;32m     58\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmip_era\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCMIP6\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfields\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     66\u001b[0m }\n\u001b[1;32m     67\u001b[0m batchsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Set your desired batch size\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m download_retraction_data(url, params, batchsize)\n",
      "Cell \u001b[0;32mIn[14], line 38\u001b[0m, in \u001b[0;36mdownload_retraction_data\u001b[0;34m(url, params, batchsize)\u001b[0m\n\u001b[1;32m     34\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batchsize\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Fetch data for each batch asynchronously\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     batch_jsons \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;241m*\u001b[39m[fetch_data(session, url, {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch}) \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches]\n\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Extract instance ids\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting instance_ids...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/cmip6-leap-feedstock-maintenance/lib/python3.10/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather\u001b[0;34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m~/miniforge3/envs/cmip6-leap-feedstock-maintenance/lib/python3.10/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m~/miniforge3/envs/cmip6-leap-feedstock-maintenance/lib/python3.10/asyncio/tasks.py:571\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cmip6-leap-feedstock-maintenance/lib/python3.10/site-packages/tqdm/asyncio.py:76\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather.<locals>.wrap_awaitable\u001b[0;34m(i, f)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_awaitable\u001b[39m(i, f):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mfetch_data\u001b[0;34m(session, url, params)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHERE\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39me)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"JSONDecodeError\") to str"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                                                   | 187/6968 [01:01<07:02, 16.04it/s]"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "\n",
    "async def fetch_data(session, url, params):\n",
    "    async with session.get(url=url, params=params) as response:\n",
    "        if response.status != 200:\n",
    "            print(params.get(\"offset\", 0))\n",
    "            print(response.status)\n",
    "            raw_response = await response.text()\n",
    "            print(raw_response)\n",
    "            try:\n",
    "                response = json.loads(raw_response)\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                print('HERE'+e)\n",
    "                \n",
    "        \n",
    "\n",
    "async def download_retraction_data(url, params, batchsize):\n",
    "    print(f\"Downloading Retraction Data from {url}...\")\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Fetch the header\n",
    "        async with session.get(url=url, params=params) as response:\n",
    "            header = await response.text()\n",
    "            header = json.loads(header)\n",
    "            n_items = header[\"response\"][\"numFound\"]\n",
    "            print(f\"Found {n_items} items.\")\n",
    "\n",
    "        # Prepare batches\n",
    "        batches = range(0, n_items + 1, batchsize)\n",
    "        params[\"limit\"] = batchsize\n",
    "\n",
    "        # Fetch data for each batch asynchronously\n",
    "        \n",
    "        batch_jsons = await tqdm.gather(\n",
    "            *[fetch_data(session, url, {**params, \"offset\": batch}) for batch in batches]\n",
    "        )\n",
    "\n",
    "    # Extract instance ids\n",
    "    print(\"Extracting instance_ids...\")\n",
    "    all_retracted_instance_ids = [\n",
    "        i[\"instance_id\"] for data in batch_jsons for i in json.loads(data)[\"response\"][\"docs\"]\n",
    "    ]\n",
    "\n",
    "    # Check if the downloaded data is complete\n",
    "    n_retracted = len(all_retracted_instance_ids)\n",
    "    if n_retracted == n_items:\n",
    "        print('Successfully downloaded all retraction info')\n",
    "    else:\n",
    "        raise RuntimeError(f'Downloaded retraction info is incomplete. ')\n",
    "\n",
    "    return all_retracted_instance_ids\n",
    "\n",
    "url = \"https://esgf-node.llnl.gov/esg-search/search\"\n",
    "params = {\n",
    "    \"type\": \"Dataset\",\n",
    "    \"mip_era\": \"CMIP6\",\n",
    "    \"replica\": \"none\",\n",
    "    \"distrib\": \"true\",\n",
    "    \"retracted\": \"true\",\n",
    "    \"format\": \"application/solr+json\",\n",
    "    \"fields\": \"instance_id\",\n",
    "}\n",
    "batchsize = 100  # Set your desired batch size\n",
    "test = await download_retraction_data(url, params, batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a678e5e4-2528-416b-aec0-f6c6882a0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "async def fetch_data(session, url, params):\n",
    "    headers = {\"Accept\": \"application/json\"}  # Specify the expected content type\n",
    "    try:\n",
    "        async with session.get(url=url, params=params, headers=headers) as response:\n",
    "            response.raise_for_status()  # Raise an exception for non-200 status codes\n",
    "            return await response.json()\n",
    "    except aiohttp.ClientResponseError as e:\n",
    "        print(f\"Error fetching batch. Status code: {e.status}\")\n",
    "        return None  # Ignore this batch\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None  # Ignore this batch\n",
    "\n",
    "async def download_data(url, params, batchsize):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Fetch the header to determine the total number of items\n",
    "        headers = {\"Accept\": \"application/json\"}  # Specify the expected content type\n",
    "        async with session.get(url=url, params=params, headers=headers) as response:\n",
    "            response.raise_for_status()  # Raise an exception for non-200 status codes\n",
    "            header = await response.json()\n",
    "            n_items = header[\"response\"][\"numFound\"]\n",
    "            print(f\"Found {n_items} items.\")\n",
    "\n",
    "        # Prepare batches\n",
    "        batches = range(0, n_items + 1, batchsize)\n",
    "        params[\"limit\"] = batchsize\n",
    "\n",
    "        # Fetch data for each batch asynchronously, handle errors, and ignore problematic batches\n",
    "        tasks = [fetch_data(session, url, {**params, \"offset\": batch}) for batch in tqdm(batches)]\n",
    "        batch_jsons = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Process the fetched data, excluding None values (error batches)\n",
    "    process_data([data for data in batch_jsons if data is not None])\n",
    "\n",
    "def process_data(batch_jsons):\n",
    "    # Extract and process the data as needed\n",
    "    # Example: Extract instance_ids\n",
    "    all_instance_ids = [\n",
    "        i[\"instance_id\"] for data in batch_jsons for i in data[\"response\"][\"docs\"]\n",
    "    ]\n",
    "    print(\"Successfully downloaded all data.\")\n",
    "    print(f\"Total items: {len(all_instance_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65473f-a634-4bee-aa68-72848cb4276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "batchsize = 100  # Adjust your desired batch size\n",
    "\n",
    "# Run the asynchronous download process\n",
    "a = await download_data(url, params, batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8cf42d-b9a3-4fdc-8511-66b1e3d0aacb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
